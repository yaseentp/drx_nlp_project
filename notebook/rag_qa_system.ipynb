{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from langchain_ollama import ChatOllama\n",
    "from scripts.chromaDB_handler import ChromaDataManager\n",
    "from scripts.config import DATA_PATH\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f288372",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"gemma3:4b-it-qat\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150da4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "# Define the input variables and template\n",
    "input_variables = ['context', 'question']\n",
    "template = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\nContext: {context} \\nAnswer:\"\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "prompt_template = PromptTemplate(input_variables=input_variables, template=template)\n",
    "\n",
    "# Define the HumanMessagePromptTemplate with the prompt template\n",
    "human_message_prompt = HumanMessagePromptTemplate(prompt=prompt_template)\n",
    "\n",
    "# Create the ChatPromptTemplate with the defined prompt and metadata\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=input_variables,\n",
    "    messages=[human_message_prompt],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a970f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_path = \"nomic-ai/nomic-embed-text-v1\"\n",
    "data_manager = ChromaDataManager(model_path=model_path, collection_name='textCollection', data_path=DATA_PATH, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: str\n",
    "    answer: str\n",
    "    tokens: int\n",
    "    tokens_per_second: float\n",
    "    history: List[dict]  # Add history to keep previous Q&A\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = data_manager.search_vector_store(state[\"question\"], n_results=1)\n",
    "    retrieved_docs.sort_values(by=['ID', 'chunk_number'], ascending=True, inplace=True)\n",
    "    retrieved_docs.reset_index(drop=True, inplace=True)\n",
    "    docs = \"\\n\".join(retrieved_docs['Text'])\n",
    "\n",
    "    # Format past history\n",
    "    history_context = \"\\n\".join([f\"User: {m['question']}\\nBot: {m['answer']}\" for m in state.get(\"history\", [])])\n",
    "    full_context = history_context + \"\\n\\n\" + docs if history_context else docs\n",
    "\n",
    "    return {\"context\": full_context}\n",
    "\n",
    "def generate(state: State):\n",
    "    messages = prompt.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": state[\"context\"]\n",
    "    })\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    usage = response.usage_metadata\n",
    "    meta = response.response_metadata\n",
    "\n",
    "    total_tokens = usage[\"total_tokens\"]\n",
    "    total_duration_sec = meta[\"total_duration\"] / 1e9\n",
    "    tokens_per_sec = round(total_tokens / total_duration_sec, 2) if total_duration_sec else 0.0\n",
    "\n",
    "    # Update chat history\n",
    "    updated_history = state.get(\"history\", []) + [{\n",
    "        \"question\": state[\"question\"],\n",
    "        \"answer\": response.content\n",
    "    }]\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"tokens\": total_tokens,\n",
    "        \"tokens_per_second\": tokens_per_sec,\n",
    "        \"history\": updated_history\n",
    "    }\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6229972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state(question: str) -> State:\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"context\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"tokens\": 0,\n",
    "        \"tokens_per_second\": 0.0,\n",
    "    }\n",
    "\n",
    "# --- Utility: run with memory ---\n",
    "def qa_chat(question: str, thread_id: str = \"default_thread\") -> State:\n",
    "    state = create_state(question)\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    return graph.invoke(state, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26554dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_chat(\"What are the two methods of calculating the total cost installed?\", thread_id=\"cost_calc\")\n",
    "\n",
    "print(\"Answer:\", response[\"answer\"])\n",
    "print(\"Tokens Used:\", response[\"tokens\"])\n",
    "print(\"Speed (tokens/sec):\", response[\"tokens_per_second\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2aafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask follow-up questions\n",
    "response2 = qa_chat(\"Explain the second method in more detail.\", thread_id=\"cost_calc\")\n",
    "print(\"Follow-up:\", response2[\"answer\"])\n",
    "print(\"Tokens Used:\", response2[\"tokens\"])\n",
    "print(\"Speed (tokens/sec):\", response2[\"tokens_per_second\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b10d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear memory\n",
    "graph.checkpointer = MemorySaver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
